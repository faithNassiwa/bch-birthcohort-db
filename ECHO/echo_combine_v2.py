# -*- coding: utf-8 -*-
"""echo_combine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cag7xEv5mxR4K4kgB_HqGTMAAVEcJVYZ
"""

import pandas as pd
import os
import logging
import numpy as np
import math

# Configure logging
logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)

# Set directory to files to get list of file_paths excluding subdirectories
directory = '/Users/faith/Desktop/ECHO/raw_downloaded/csv_files/'
file_paths = [os.path.join(directory, file) for file in os.listdir(directory) if
              os.path.isfile(os.path.join(directory, file))]
maternal_nutrition_paper_file_paths = ['/Users/faith/Desktop/Work/Boston Childrens Hospital/birth-cohort-db/ECHO/data_dump/Data_CSVDownloaded/01_research/ptReg.csv', '/Users/faith/Desktop/Work/Boston Childrens Hospital/birth-cohort-db/ECHO/data_dump/Data_CSVDownloaded/02_forms/Ess_CHB_BLOCK2.csv', '/Users/faith/Desktop/Work/Boston Childrens Hospital/birth-cohort-db/ECHO/data_dump/Data_CSVDownloaded/02_forms/Ess_CHB_CFSP.csv', '/Users/faith/Desktop/Work/Boston Childrens Hospital/birth-cohort-db/ECHO/data_dump/Data_CSVDownloaded/02_forms/Ess_Prg_MMRAaG.csv', '/Users/faith/Desktop/Work/Boston Childrens Hospital/birth-cohort-db/ECHO/data_dump/Data_CSVDownloaded/02_forms/Ess_CHB_BLOCK3.csv', '/Users/faith/Desktop/Work/Boston Childrens Hospital/birth-cohort-db/ECHO/data_dump/Data_CSVDownloaded/02_forms/Ess_Prg_MSupp_R.csv', '/Users/faith/Desktop/Work/Boston Childrens Hospital/birth-cohort-db/ECHO/data_dump/Data_CSVDownloaded/02_forms/Rec_RCh_ASA_TS.csv', '/Users/faith/Desktop/Work/Boston Childrens Hospital/birth-cohort-db/ECHO/data_dump/Data_CSVDownloaded/02_forms/Ess_CHB_DSQ_SR2.csv', '/Users/faith/Desktop/Work/Boston Childrens Hospital/birth-cohort-db/ECHO/data_dump/Data_CSVDownloaded/02_forms/Ess_Prg_MSupp_PI.csv', '/Users/faith/Desktop/Work/Boston Childrens Hospital/birth-cohort-db/ECHO/data_dump/Data_CSVDownloaded/02_forms/Rec_RCh_ASA_TNS.csv', '/Users/faith/Desktop/Work/Boston Childrens Hospital/birth-cohort-db/ECHO/data_dump/Data_CSVDownloaded/02_forms/Ess_Prg_MSuppSF_R.csv', '/Users/faith/Desktop/Work/Boston Childrens Hospital/birth-cohort-db/ECHO/data_dump/Data_CSVDownloaded/02_forms/Ess_Prg_MFSP.csv', '/Users/faith/Desktop/Work/Boston Childrens Hospital/birth-cohort-db/ECHO/data_dump/Data_CSVDownloaded/02_forms/Ess_Prg_DSQ_SR.csv', '/Users/faith/Desktop/Work/Boston Childrens Hospital/birth-cohort-db/ECHO/data_dump/Data_CSVDownloaded/02_forms/Ess_Prg_MMRA2.csv', '/Users/faith/Desktop/Work/Boston Childrens Hospital/birth-cohort-db/ECHO/data_dump/Data_CSVDownloaded/02_forms/Rec_RCh_ASA_Totals.csv', '/Users/faith/Desktop/Work/Boston Childrens Hospital/birth-cohort-db/ECHO/data_dump/Data_CSVDownloaded/02_forms/Ess_Prg_MSupp_PP.csv', '/Users/faith/Desktop/Work/Boston Childrens Hospital/birth-cohort-db/ECHO/data_dump/Data_CSVDownloaded/02_forms/Ess_CHB_DSQ_PR.csv', '/Users/faith/Desktop/Work/Boston Childrens Hospital/birth-cohort-db/ECHO/data_dump/Data_CSVDownloaded/02_forms/Ess_Prg_MSuppSF_PP.csv', '/Users/faith/Desktop/Work/Boston Childrens Hospital/birth-cohort-db/ECHO/data_dump/Data_CSVDownloaded/02_forms/Ess_CHB_IFP.csv', '/Users/faith/Desktop/Work/Boston Childrens Hospital/birth-cohort-db/ECHO/data_dump/Data_CSVDownloaded/02_forms/Ess_CHB_CFH.csv', '/Users/faith/Desktop/Work/Boston Childrens Hospital/birth-cohort-db/ECHO/data_dump/Data_CSVDownloaded/02_forms/Ess_CHB_BLOCK.csv', '/Users/faith/Desktop/Work/Boston Childrens Hospital/birth-cohort-db/ECHO/data_dump/Data_CSVDownloaded/02_forms/Ess_Prg_MMRA.csv']

# Make PtReg file the first item in the list since we shall need it to be set as the base with all the participant
# registrations
PtReg = os.path.join(directory, 'PtReg.csv')
file_paths.insert(0, file_paths.pop(file_paths.index(PtReg)))
len(file_paths)
# Specify the columns you want to group by
group_columns = ['xCohortID', 'xParticipantID']
drop_columns = ['ProtocolID', 'xSiteID']


# Define the aggregation functions
def custom_agg(series):
    # Filter out NaN values before joining
    return ', '.join(series.dropna().astype(str))


def list_agg(series):
    return list(series.dropna())


def aggregate_data(grouping_columns, grouping_df):
    # Create an empty aggregation dictionary
    agg_dict = {}

    # Dynamically add columns to the dictionary with the appropriate function
    for col in grouping_df.columns:
        if col in grouping_columns:
            continue  # Skip group by columns
        elif grouping_df[col].dtype in [np.float64, np.int64]:
            agg_dict[col] = list_agg  # Use list aggregation for numeric columns

        else:
            agg_dict[col] = custom_agg  # Apply custom concatenation

    # Apply the aggregation to the DataFrame
    grouped_df = grouping_df.groupby(group_columns).agg(agg_dict)

    # Flatten the multi-level columns if necessary
    grouped_df.columns = ['_'.join(col) if isinstance(col, tuple) else col for col in grouped_df.columns.values]

    grouped_df.reset_index(inplace=True)
    return grouped_df


def combine_echo_files(files):
    # Initialize an empty DataFrame and trackers
    base_df = pd.DataFrame()
    file_count = 0
    merged_files = []
    # Process each file
    for index, file in enumerate(files):
        if file.endswith('.csv'):
            # Read the current file into a DataFrame
            temp_df = pd.read_csv(file, low_memory=False)

            # Dynamic suffixes based on the file names
            base_file = files[index - 1].replace(directory, '')
            base_form_name = os.path.splitext(base_file)[0]
            temp_file = file.replace(directory, '')
            temp_form_name = os.path.splitext(temp_file)[0]
            suffixes = (f'_{base_form_name}', f'_{temp_form_name}')

            logging.info('Reading file at %s, Suffix: %s', file, suffixes)

            # Add the first file(PtReg)
            if base_df.empty:
                base_df = temp_df
            else:
                # Remove columns that are repeated and/or not relevant
                temp_df = temp_df.drop(columns=[col for col in drop_columns if col in temp_df.columns])

                # Aggregate the data in the form to combine with the PtReg
                temp_df = aggregate_data(group_columns, temp_df)

                # Merge the new DataFrame with the base DataFrame on the join key
                base_df = pd.merge(base_df, temp_df, on=['xCohortID', 'xParticipantID'], how='left', suffixes=suffixes)
        file_count += 1
        merged_files.append(file)
        logging.info(f'Files processed: {file_count}')
    # Remove PtReg file from merged files so that it can be used as base in next batch
    merged_files.remove(PtReg)
    return base_df, merged_files, file_count


# Function to process files in batches
def process_files_in_batches(file_paths, batch_size, base_file=PtReg, start_batch_index=0):
    total_files = len(file_paths)
    num_batches = math.ceil(total_files / batch_size)

    for i in range(start_batch_index, num_batches):
        start_index = i * batch_size
        end_index = start_index + batch_size
        batch_files = file_paths[start_index:end_index]

        # Ensure the base file is in the current batch
        if base_file not in batch_files:
            # Insert the file to always-include at the start of the batch list
            batch_files.insert(0, base_file)

        combined_df, merged_files, files_count = combine_echo_files(batch_files)
        combined_df.to_csv(f'combined_echo_data_batch_{i+1}.csv', index=False)
        logging.info(f"Batch {i+1}: {files_count} files merged.")

    logging.info("All batches completed!")


if __name__ == '__main__':
    process_files_in_batches(file_paths, 50)



#%%
