# -*- coding: utf-8 -*-
"""echo_combine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TpwXvfnn-RqUMP9bJxY6dOJImjMavLWI
"""

import pandas as pd
import os
import logging

# Logging events
logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)

# Set directory to files to get list of file_paths excluding subdirectories
directory = '/Users/faith/Desktop/ECHO/raw_downloaded/csv_files'
file_paths = [os.path.join(directory, file) for file in os.listdir(directory) if os.path.isfile(os.path.join(directory, file))]

# Make PtReg file the first item in the list since we shall need it to be set as the base with all the participant registrations
PtReg = os.path.join(directory, 'PtReg.csv')
file_paths.insert(0, file_paths.pop(file_paths.index(PtReg)))
len(file_paths)

def combine_echo_files(files):
    # Initialize an empty DataFrame and a set for tracking columns
    base_df = pd.DataFrame()
    seen_columns = set()
    file_count = 0
    # Process each file
    for file in files:
        # Read the current file into a DataFrame
        temp_df = pd.read_csv(file, low_memory=False)
        logging.info('Reading %s', file)

        # For the first file(PtReg), add all its columns to the seen set and update base_df
        if base_df.empty:
            seen_columns.update(temp_df.columns)
            base_df = temp_df

        else:
            # Identify columns in temp_df that are not in seen_columns
            new_columns = [col for col in temp_df.columns if col not in seen_columns]

            # Update seen_columns with the new columns
            seen_columns.update(new_columns)

            # Select only the new columns (to avoid duplicates) + the join key(s) for merging
            temp_df = temp_df[new_columns + ['ProtocolID', 'xCohortID', 'xParticipantID']]

            # Merge the new DataFrame with the base DataFrame on the join key
            base_df = pd.merge(base_df, temp_df, on=[ 'ProtocolID', 'xCohortID', 'xParticipantID'], how='outer')
        file_count += 1
        logging.info('Files processed: %s', file_count)

    return base_df

combined_df = combine_echo_files(file_paths) # file Rec_RCh_NDSR02_C.csv is large file number 54 and takes a while to execute ... over an hour on local computer resources

#Save the combined DataFrame to a new CSV file
combined_df.to_csv('combined_echo_data.csv', index=False)




def dictread():
    # Read each CSV into a DataFrame, ensuring all potentially present columns are considered
    dataframes = []
    for file in csv_files:
        df = pd.read_csv(file)
        dataframes.append(df)

    # Concatenate all the dataframes
    combined_df = pd.concat(dataframes, ignore_index=True)

    # Assuming 'ID', 'Name', and 'Date' are your key columns to uniquely identify records
    key_columns = ['ProtocolID', 'xCohortID', 'xParticipantID']

    # Fill NaN values for easier manipulation
    combined_df.fillna('', inplace=True)

    # Use a dictionary to gather lists of values for each unique key combination
    from collections import defaultdict
    data_dict = defaultdict(lambda: defaultdict(list))

    # Populate the dictionary
    for idx, row in combined_df.iterrows():
        key = tuple(row[col] for col in key_columns)
        for col in combined_df.columns:
            if col not in key_columns:
                data_dict[key][col].append(row[col])

    # Optionally convert defaultdict to regular dict if necessary
    data_dict = dict(data_dict)

    return data_dict



